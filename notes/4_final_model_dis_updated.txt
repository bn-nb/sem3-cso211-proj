\section{LIMITATIONS OF MODEL-CENTRIC AI}

Model-centric AI is predicated on machine learning approaches that prioritize improving model architectures (algorithm/code) as well as the underlying hyper-parameters. In this method, data is created essentially just once and is maintained throughout the development of the AI system. Model-centric AI has been successful over the past few decades, yet it has an unfixable flaw. It particularly thrives in organizations and sectors when there are customer platforms with millions of users are free to depend on  generic fixes. In these conditions, the majority of consumers would be satisfied by a single AI system, nonetheless, outliers would be practically useless. Examples of such organizations and sectors include the advertising sector, where firms like Google, Baidu, Amazon, and Facebook have access to vast amounts of data (sometimes in a standardized format), which they may use to build model-centric AI systems. Standardized solutions like those offered by a single AI system cannot be used in sectors like manufacturing, agriculture, or healthcare where customized solutions are preferred versus one-size-fits-all recipes. Instead, they should conceive their strategy to ensure that their model's (algorithm's) model learns what it needs to learn from having complete data that includes all crucial cases and is labelled consistently.

Due to the rapid pace of innovation in today's technology-driven world, AI models and the algorithms they are modelled after may quickly become outdated and require retraining on new data, primarily because significant trends' unforeseen or unexpected behaviour may outpace the model's usefulness and relevance. Such model-centric AI's capabilities are severely constrained in fields where the model itself is created to support ongoing scientific study or as a component of a more complex model. As a result, training data is produced using a smaller model based on the restricted findings and hypotheses. Typically, data collection is not practicable in these situations, or work involves limited data from expensive datasets. In such cases, the shortcomings of Model-Centric technologies become apparent, since training a model-centred algorithm may result in biased findings, mainly if the initial model used to generate the data was built on incomplete knowledge or under a lack of domain expertise. With general trends in Deep-Learning requiring focus on large amounts of data, there is a general tendency to prefer reusing existing models to fit into our use cases with specific training data. Such a shift is perceived as a step towards the Data-Centric movement, where the model is fixed, with the only variable inputs being the training data and classifiers.
