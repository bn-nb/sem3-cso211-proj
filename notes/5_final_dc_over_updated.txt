\section{AN OVERVIEW OF DATA-CENTRIC AI}

Introduced to the mainstream by the co-founder of DeepLearning.AI and adjunct Professor at Stanford University, Andrew Ng, on March 24, 2021, data-centric AI is a data-engineering strategy that tries to improve an AI system's performance by methodically boosting the quality of the data used to train the underlying model, under mutually exclusive yet collectively exhaustive methodologies and categories. Data-centric AI can improve the performance of AI models and services through augmentation, extrapolation, and interpolation. Data-centric AI can assist in making AI services more accurate and dependable by expanding the data that is accessible to them and enabling them to use it more efficiently. With the help of training data from many sources, including synthetic data, public data sets, and private data sets, data-centric AI is created utilizing this innovative methodology. This strategy can lessen the time and effort needed to generate training data while also helping to increase the quality of the data. It can also help increase the effectiveness with which training data is used by AI services. Additionally, data-centric AI can process additional data sets because the data is personalized. This means that regardless of the magnitude of the data set, data-centric AI can analyze and learn from it and make decent predictions. 

Furthermore, data-centric AI is not limited to a specific type of data. It can learn from text, images, audio, and video. In general, a data-centric AI strategy comprises using the appropriate labels and fixing any problems, getting rid of noisy data inconsistencies, data augmentation, feature engineering, analysis of errors employing domain experts to identify the accuracy or inaccuracy in data points, etc. Data-centric AI constantly evaluates the created AI model in conjunction with updated data. A developed AI model would typically only train on a data set once during the production stage before the software development process can be completed with the deployment of the model with required functionality. The underlying model will eventually come across edge-case instances of data points that are entirely different from those encountered during the training phase. This behaviour is expected as the workflow of data-centric AI is assumed to include the successive improvement of data, in particular, in businesses and industries that cannot afford to have enormous data points (e.g., manufacturing, agriculture, and healthcare). As a result, evaluating the model's quality would also happen more regularly than only once. A model would be able to recognize, judge and then answer back appropriately to variational data distributions owing to the production systems' capacity to offer rapid feedback. In fact, this ability gives data-centric approaches a competitive advantage over their model-centric counterparts.

The ever-arising challenges and problems in today's world require continuous optimization and tuning of the model, along with simultaneous collection, processing, augmentation, and labelling of high-volume data. In cases of filter-list-based blocking/ moderation of social media content, restriction of cyber-threats, fraud detection, and spam tracking, a shift from a data-driven or application-centric to a data-centric perspective focusing on labelling and cleaning of data is apparent, with the information being collected in high frequency, at an hourly basis or even faster. The boundaries between business and technology are vanishing, with tools and techniques like ML and DL requiring assistance from domain experts and consultants to modify inputs or generate better algorithms. DL or Deep-Learning based approaches have become popular owing to the enormous scope and capability for collecting, storing, and processing Big Data, mainly due to their excellent performance with big data and technological advances. 

In Deep-Learning based applications, the distinguishing hierarchy and structure of features or parameters are learned from the data. At the same time, they are usually coded by a human domain expert in typical machine learning applications. Then, through algorithms such as gradient descent and backpropagation, the deep learning algorithm learns and fits itself for accuracy. This methodology allows for mimicking the human brain at a primitive level, allowing DL models to make predictions more precisely through a combination of weights, inputs, and biases. Since a massive volume of data is processed through multiple layers of neural networks, the aspect of clean, labelled information becomes vital, as the presence of dirty, repeated, inconsistent data can cause unnatural biases, failure in edge cases, erroneous predictions, the poor performance of the model, wasteful computations, etc. Most real-world datasets are noisy, unstructured, and unorganized, with several biases, outliers, missing values, repeated values, etc.

To address this issue from a data-centric standpoint when developing their AI systems, businesses and industries need to pay more attention to guaranteeing a consistent collection of high-quality data sets than treating this vital product development phase as a one-time task.

1) Limited Data: The crucial issue is the need for large data sets that are both type-intensive and quantity-comprehensive. Contrary to Internet corporations (like Google and Baidu), the amount of data that manufacturing industries have access to is frequently constrained. Typically, they use data sets comprising 102â€“103 pertinent data points to train their models. As a result, when using approaches designed for hundreds of millions of data points, a model trained on no more than 103 relevant cases to detect some flaw (or a rare disease) will struggle.

2) Solution Customization: Highly individualized solutions are also necessary. Think about a manufacturing company that sells a variety of goods. Since each manufactured product would require a uniquely trained ML system. The idea of a single universal AI system for flaw identification and classification across every item would not be effective.
